import os
import re
import time
import json
import csv
import pandas as pd
from datetime import datetime
from dotenv import load_dotenv
import openai
from firecrawl import Firecrawl

# ========== âš™ï¸ ç¯å¢ƒé…ç½® ==========
# åœ¨åŒç›®å½•ä¸‹åˆ›å»º .env æ–‡ä»¶ï¼š
# OPENAI_API_KEY=sk-xxxx
# FIRECRAWL_API_KEY=fc-xxxx
load_dotenv()

OPENAI_API_KEY = ""
FIRECRAWL_API_KEY = ""

# ========== âš™ï¸ å¸¸é‡é…ç½® ==========
MODEL = "gpt-5"
EXCEL_FILE = "prompts.xlsx"
OUTPUT_FILE = "outputs_etsy/openai_firecrawl_summary.csv"
BASE_DIR = "outputs_etsy"
MAX_URLS = 3
MAX_RETRIES = 3
RETRY_DELAY = 3  # ç§’
TIMEOUT = 300000    # Firecrawlè¶…æ—¶ï¼ˆæ¯«ç§’ï¼‰

os.makedirs(BASE_DIR, exist_ok=True)
openai.api_key = OPENAI_API_KEY

# åˆå§‹åŒ– Firecrawl SDK
firecrawl = Firecrawl(api_key=FIRECRAWL_API_KEY)


# ========== Step 1ï¸âƒ£ GPT æœç´¢ Etsy ç½‘é¡µ ==========
def get_urls_from_openai(prompt: str, model=MODEL, max_urls=MAX_URLS):
    print(f"\nğŸ§  è°ƒç”¨ GPT WebSearch: {prompt[:60]}...")
    try:
        response = openai.responses.create(
            model=model,
            tools=[{"type": "web_search_preview", "search_context_size": "high"}],
            input=prompt + "\nReturn 5 product URLs with full https:// links."
        )

        text_output = ""
        for item in getattr(response, "output", []) or []:
            if getattr(item, "type", "") == "message":
                for content in getattr(item, "content", []) or []:
                    if getattr(content, "type", "") == "output_text":
                        text_output += getattr(content, "text", "")

        urls = re.findall(r"https?://[^\s)\"'>]+", text_output)
        urls = [u.rstrip(".,)]") for u in urls]
        print(f"ğŸ”— æå–åˆ° {len(urls)} ä¸ªé“¾æ¥")
        return urls[:max_urls], text_output
    except Exception as e:
        print(f"âŒ GPT è°ƒç”¨å¤±è´¥: {e}")
        return [], ""


# ========== Step 2ï¸âƒ£ Firecrawl SDK æŠ“å– HTML + Markdown + å®Œæ•´ JSON ==========
def fetch_with_firecrawl(url, prompt_dir):
    """
    ä½¿ç”¨ Firecrawl SDK æŠ“å– HTML + Markdown + å®Œæ•´ JSONï¼ˆç©º schemaï¼‰
    å¢å¼ºç‰ˆï¼š
    âœ… åŠ¨æ€å»¶è¿Ÿï¼ˆé˜²æ­¢è§¦å‘é™æµï¼‰
    âœ… æ™ºèƒ½é‡è¯•ï¼ˆé€æ¬¡å»¶é•¿ç­‰å¾…æ—¶é—´ï¼‰
    âœ… è‡ªåŠ¨å»¶é•¿ timeout
    âœ… æ£€æµ‹é˜²çˆ¬é¡µé¢ï¼ˆ"Please enable JS"ï¼‰
    """
    import random

    safe_name = re.sub(r"[^a-zA-Z0-9_-]", "_", url[:80])
    md_path = os.path.join(prompt_dir, f"{safe_name}.md")
    html_path = os.path.join(prompt_dir, f"{safe_name}.html")
    json_path = os.path.join(prompt_dir, f"{safe_name}_full.json")

    base_timeout = TIMEOUT  # åˆå§‹è¶…æ—¶ï¼ˆæ¯«ç§’ï¼‰
    for attempt in range(1, MAX_RETRIES + 1):
        print(f"\nğŸ•¸ï¸ Firecrawl æŠ“å–ç¬¬ {attempt}/{MAX_RETRIES} æ¬¡: {url}")
        try:
            # ========== è®¡æ—¶ ==========
            start = time.time()

            # ========== æ‰§è¡ŒæŠ“å– ==========
            result = firecrawl.scrape(
                url,
                formats=[
                    "markdown",
                    "html",
                    {
                        "type": "json",
                        "schema": {"type": "object", "properties": {}}
                    }
                ],
                only_main_content=False,  # âœ… åªæŠ“æ­£æ–‡éƒ¨åˆ†ï¼Œæé€Ÿæ˜¾è‘—
                timeout=base_timeout
            )

            elapsed = round(time.time() - start, 2)

            # ========== æ£€æŸ¥æ˜¯å¦è¢«é˜²çˆ¬ ==========
            if result.html and "Please enable JS" in result.html:
                print("âš ï¸ æ£€æµ‹åˆ°åçˆ¬é¡µé¢ï¼ˆéœ€è¦å¯ç”¨ JSï¼‰ï¼Œå‡†å¤‡å»¶è¿Ÿåé‡è¯•...")
                # å¢åŠ éšæœºå»¶è¿Ÿåé‡è¯•
                delay = random.uniform(10, 20)
                print(f"â³ ç­‰å¾… {round(delay, 1)} ç§’å†è¯•...")
                time.sleep(delay)
                continue

            # ========== ä¿å­˜æ–‡ä»¶ ==========
            with open(md_path, "w", encoding="utf-8") as f:
                f.write(result.markdown or "")
            with open(html_path, "w", encoding="utf-8") as f:
                f.write(result.html or "")
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(result.json or {}, f, ensure_ascii=False, indent=2)

            print(f"âœ… æŠ“å–æˆåŠŸ [{elapsed}s] å·²ä¿å­˜ Markdown / HTML / JSON")
            return {
                "url": url,
                "status": "success",
                "markdown_file": md_path,
                "html_file": html_path,
                "json_file": json_path,
                "attempt": attempt,
                "elapsed_s": elapsed,
                "time_s": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }

        except Exception as e:
            print(f"âŒ Firecrawl æŠ“å–å¤±è´¥: {e}")

            # ========== åŠ¨æ€å»¶è¿Ÿ + Timeout è°ƒæ•´ ==========
            delay = RETRY_DELAY * attempt + random.uniform(2, 5)
            base_timeout = int(base_timeout * 1.5)  # æ¯æ¬¡è¶…æ—¶åè‡ªåŠ¨å»¶é•¿ 1.5 å€
            print(f"âš™ï¸ ç¬¬ {attempt} æ¬¡å¤±è´¥ï¼Œ{round(delay, 1)} ç§’åé‡è¯•ï¼Œ"
                  f"æ–°çš„ timeout={base_timeout/1000:.1f}s")

            time.sleep(delay)

    print("âŒ Firecrawl å…¨éƒ¨é‡è¯•å¤±è´¥")
    return {"url": url, "status": "fail"}


# ========== Step 3ï¸âƒ£ ä¸»æµç¨‹ ==========
def main():
    if not os.path.exists(EXCEL_FILE):
        print("âŒ æœªæ‰¾åˆ° Excel æ–‡ä»¶ã€‚")
        return

    df = pd.read_excel(EXCEL_FILE, engine="openpyxl")
    prompts = df["Prompt"].dropna().tolist() if "Prompt" in df.columns else df.iloc[:, 1].dropna().tolist()
    all_logs = []

    for idx, prompt in enumerate(prompts, 1):
        print(f"\n========== ğŸ” Prompt {idx}/{len(prompts)} ==========")
        prompt_name = re.sub(r"[^a-zA-Z0-9_()ï¼ˆï¼‰\u4e00-\u9fa5-]", "_", prompt[:50])
        prompt_dir = os.path.join(BASE_DIR, prompt_name)
        os.makedirs(prompt_dir, exist_ok=True)

        urls, gpt_output = get_urls_from_openai(prompt)
        if not urls:
            print("âš ï¸ GPT æœªè¿”å› URLï¼Œè·³è¿‡ã€‚")
            continue

        for url in urls:
            result = fetch_with_firecrawl(url, prompt_dir)
            result.update({
                "prompt": prompt,
                "prompt_dir": prompt_dir,
                "gpt_raw_output": gpt_output
            })
            all_logs.append(result)
            time.sleep(1)

    # è¾“å‡ºç»“æœ CSV
    df_out = pd.DataFrame(all_logs)
    df_out.to_csv(OUTPUT_FILE, index=False, encoding="utf-8-sig", quoting=csv.QUOTE_ALL)
    print(f"\nâœ… å·²ä¿å­˜ç»“æœï¼š{OUTPUT_FILE}")
    print(f"ğŸ“ å„ prompt æ–‡ä»¶ä¿å­˜åœ¨ï¼š{BASE_DIR}")


if __name__ == "__main__":
    main()
