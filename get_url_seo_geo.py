import os
import re
import time
import json
import random
from datetime import datetime

import pandas as pd
from dotenv import load_dotenv
from firecrawl import Firecrawl
from bs4 import BeautifulSoup
import openai

from pydantic import BaseModel, Field
from typing import Optional, List

# ================================================================
# ç¯å¢ƒ & å…¨å±€é…ç½®
# ================================================================
load_dotenv()

FIRECRAWL_API_KEY = ""
OPENAI_API_KEY = ""

if not FIRECRAWL_API_KEY:
    raise ValueError("âŒ è¯·åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½® FIRECRAWL_API_KEY")
if not OPENAI_API_KEY:
    print("âš ï¸ æœªæ£€æµ‹åˆ° OPENAI_API_KEYï¼Œåªèƒ½è¿è¡Œ SEO çˆ¬å–ï¼ˆGEO ä¼šè·³è¿‡ï¼‰")

openai.api_key = OPENAI_API_KEY

# Firecrawl å®¢æˆ·ç«¯
firecrawl = Firecrawl(api_key=FIRECRAWL_API_KEY)

# ç›®å½• / æ–‡ä»¶é…ç½®
EXCEL_FILE = "prompts.xlsx"
SEO_BASE_DIR = "outputs_etsy_final"      # å¯¹åº”ä»»åŠ¡ä¸­çš„ SEO ç›®å½•
GEO_BASE_DIR = "outputs_GEO_final"     # å¯¹åº”ä»»åŠ¡ä¸­çš„ GEO ç›®å½•

os.makedirs(SEO_BASE_DIR, exist_ok=True)
os.makedirs(GEO_BASE_DIR, exist_ok=True)

# æŠ“å–å‚æ•°
MAX_LISTINGS_PER_QUERY = 10      # ä»»åŠ¡è¦æ±‚ï¼šæ¯ä¸ª query Ã— channel 3 ä¸ªç»“æœ â†’ 50*3*2=300
MAX_RETRIES = 3
RETRY_DELAY = 3   # ç§’
TIMEOUT = 300000  # msï¼ŒFirecrawl timeout

# OpenAI æ¨¡å‹ï¼ˆä½ å¯ä»¥æŒ‰è‡ªå·±ç¯å¢ƒæ”¹ï¼‰
OPENAI_MODEL = os.environ.get("OPENAI_MODEL", "gpt-5")


# ================================================================
# Pydantic Schemaï¼šç”¨äº Firecrawl JSON ç»“æ„åŒ–è¾“å‡º
# ================================================================
class EtsyImage(BaseModel):
    """æè¿°å•ä¸ªå•†å“å›¾ç‰‡åŠå…¶å…ƒæ•°æ®"""
    url: Optional[str] = Field(None, description="The full URL of the product image.")
    alt_text: Optional[str] = Field(None, description="The image's alt text for SEO and accessibility.")


class EtsyProduct(BaseModel):
    """Etsy å•†å“é¡µé¢çš„ç»“æ„åŒ–æ•°æ®æ¨¡å‹"""

    # A. Basic product attributes
    title: Optional[str] = None
    price: Optional[str] = None
    originalPrice: Optional[str] = None
    rating: Optional[str] = None
    reviewsCount: Optional[str] = None
    bestseller: Optional[bool] = None
    starSeller: Optional[bool] = None

    # B. Shop metadata
    shop: Optional[str] = None
    delivery: Optional[str] = None
    shopPolicies: Optional[str] = None
    purchaseProtection: Optional[str] = None
    return_policy_text: Optional[str] = None
    paymentMethods: Optional[List[str]] = None

    # C. Images
    images: Optional[List[EtsyImage]] = None

    # D. Description
    description_text: Optional[str] = None

    # E. FAQ
    faq_items: Optional[List[str]] = None


# ================================================================
# å·¥å…·å‡½æ•°ï¼šå®‰å…¨å‘½å
# ================================================================
def safe_query_name(prompt: str) -> str:
    """
    ç”¨äºç”Ÿæˆ query å­ç›®å½•åï¼š
    - ä¿ç•™å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ã€éƒ¨åˆ†æ‹¬å·å’Œä¸­æ–‡
    - å…¶ä»–å­—ç¬¦æ›¿æ¢ä¸º "_"
    """
    return re.sub(r"[^a-zA-Z0-9_()ï¼ˆï¼‰\u4e00-\u9fa5-]", "_", prompt[:50])


def safe_slug_from_url(url: str, max_len: int = 80) -> str:
    """
    æ ¹æ® URL ç”Ÿæˆæ–‡ä»¶å slugï¼Œé¿å…ç‰¹æ®Šå­—ç¬¦ã€‚
    """
    return re.sub(r"[^a-zA-Z0-9_-]", "_", url[:max_len])


# ================================================================
# SEO æ¸ é“ï¼šç›´æ¥ç”¨ Etsy æœç´¢é¡µé¢æå– listing URL
# ================================================================
def get_seo_urls_from_etsy(prompt: str, max_urls: int = MAX_LISTINGS_PER_QUERY):
    query_param = prompt.replace(" ", "+")
    search_url = f"https://www.etsy.com/search?q={query_param}"
    print(f"\nğŸ” [SEO] Etsy æœç´¢ URL: {search_url}")

    urls = []
    try:
        result = firecrawl.scrape(
            search_url,
            formats=["html"],
            only_main_content=False,
            timeout=TIMEOUT
        )
        html = result.html or ""
        # æå– listing URL
        urls = re.findall(r'https://www\.etsy\.com/listing/[0-9]+/[^\s"\'<>]+', html)
        # å»é‡ï¼Œä¿ç•™å‰ max_urls ä¸ª
        urls = list(dict.fromkeys(urls))[:max_urls]
        print(f"ğŸ”— [SEO] æå–åˆ° {len(urls)} ä¸ªå•†å“é“¾æ¥")
    except Exception as e:
        print(f"âŒ [SEO] Etsy æœç´¢æŠ“å–å¤±è´¥: {e}")

    return urls, search_url


# ================================================================
# GEO æ¸ é“ï¼šä½¿ç”¨ OpenAI WebSearch è·å– Etsy listing URL
# ================================================================
def get_geo_urls_from_openai(prompt: str, max_urls: int = MAX_LISTINGS_PER_QUERY):
    if not OPENAI_API_KEY:
        print("âš ï¸ æœªé…ç½® OPENAI_API_KEYï¼Œè·³è¿‡ GEO URL è·å–")
        return [], ""

    full_prompt = (
        prompt
        + "\nSearch the web and return ONLY a list of Etsy product URLs."
          "\nDo NOT return JSON."
          "\nDo NOT return structured data."
          "\nReturn one URL per line, each starting with https://."
    )

    print(f"\nğŸ§  [GEO] GPT WebSearch for prompt: {prompt[:60]}")

    raw_text = ""
    urls = []

    try:
        # è¿™é‡Œæ²¿ç”¨ä½ ä¹‹å‰çš„ Responses API è°ƒç”¨é£æ ¼
        response = openai.responses.create(
            model=OPENAI_MODEL,
            tools=[{"type": "web_search_preview"}],
            input=full_prompt,
        )

        # è§£ææ–‡æœ¬è¾“å‡ºéƒ¨åˆ†ï¼ˆæ ¹æ®ä½ ä¹‹å‰çš„ä»£ç é£æ ¼ï¼‰
        for item in response.output:
            if item.type == "message":
                for block in item.content:
                    if block.type == "output_text":
                        raw_text += block.text

        # ä» raw_text ä¸­æå– Etsy listing URL
        urls = [
            u for u in re.findall(r"https?://[^\s\"'>]+", raw_text)
            if "etsy.com" in u and "/listing/" in u
        ]
        urls = list(dict.fromkeys(urls))[:max_urls]

        print(f"ğŸ”— [GEO] GPT è¿”å› {len(urls)} ä¸ªå•†å“é“¾æ¥")
    except Exception as e:
        print(f"âŒ [GEO] GPT WebSearch å‡ºé”™ï¼š{e}")

    return urls, raw_text


# ================================================================
# Firecrawl å•é¡µæŠ“å–ï¼šMarkdown / HTML / JSON
# ================================================================
def fetch_with_firecrawl(url: str, out_dir: str, channel: str, query_id: str, rank: int):
    """
    å¯¹å•ä¸ªå•†å“é¡µè¿›è¡ŒæŠ“å–ï¼Œä¿å­˜ md / html / jsonã€‚
    è¿”å›ä¸€æ¡æ—¥å¿— dictã€‚
    """
    os.makedirs(out_dir, exist_ok=True)
    slug = safe_slug_from_url(url)
    md_path = os.path.join(out_dir, f"{slug}.md")
    html_path = os.path.join(out_dir, f"{slug}.html")
    json_path = os.path.join(out_dir, f"{slug}_full.json")

    timeout = TIMEOUT
    base_timeout = timeout

    for attempt in range(1, MAX_RETRIES + 1):
        print(f"\nğŸ•¸ï¸ [{channel}] Firecrawl æŠ“å– {attempt}/{MAX_RETRIES}: {url}")
        start_time = time.time()

        try:
            result = firecrawl.scrape(
                url,
                formats=[
                    "markdown",
                    "html",
                    {
                        "type": "json",
                        "schema": EtsyProduct.model_json_schema()
                    }
                ],
                only_main_content=False,
                timeout=timeout
            )

            elapsed = round(time.time() - start_time, 2)

            # åçˆ¬æ£€æµ‹ç¤ºä¾‹ï¼šå¦‚æœçœ‹åˆ°éœ€è¦å¯ç”¨ JS çš„æç¤ºï¼Œå°± retry
            if result.html and "Please enable JS" in result.html:
                print("âš ï¸ æ£€æµ‹åˆ°å¯èƒ½çš„åçˆ¬é¡µé¢ï¼Œç­‰å¾…åé‡è¯•...")
                delay = random.uniform(10, 20)
                print(f"â³ ç­‰å¾… {round(delay, 1)} ç§’...")
                time.sleep(delay)
                continue

            # ä¿å­˜æ–‡ä»¶
            with open(md_path, "w", encoding="utf-8") as f:
                f.write(result.markdown or "")
            with open(html_path, "w", encoding="utf-8") as f:
                f.write(result.html or "")
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(result.json or {}, f, ensure_ascii=False, indent=2)

            print(f"âœ… [{channel}] æŠ“å–æˆåŠŸ ({elapsed}s) â†’ {slug}")
            return {
                "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "channel": channel,
                "query_id": query_id,
                "rank": rank,
                "url": url,
                "url_slug": slug,
                "status": "success",
                "md_path": md_path,
                "html_path": html_path,
                "json_path": json_path,
                "attempt": attempt,
                "elapsed_s": elapsed,
            }

        except Exception as e:
            elapsed = round(time.time() - start_time, 2)
            print(f"âŒ [{channel}] Firecrawl æŠ“å–å¤±è´¥ (ç¬¬ {attempt} æ¬¡) [{elapsed}s]: {e}")
            # çº¿æ€§ + éšæœºå»¶æ—¶ï¼Œtimeout é€’å¢
            delay = RETRY_DELAY * attempt + random.uniform(1, 3)
            timeout = int(timeout * 1.5)
            print(f"âš™ï¸ ç­‰å¾… {round(delay, 1)} ç§’åé‡è¯•ï¼Œæ–°çš„ timeout = {timeout/1000:.1f} ç§’")
            time.sleep(delay)

    print(f"ğŸ›‘ [{channel}] Firecrawl å¤šæ¬¡é‡è¯•ä»å¤±è´¥: {url}")
    return {
        "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "channel": channel,
        "query_id": query_id,
        "rank": rank,
        "url": url,
        "status": "fail"
    }


# ================================================================
# ä¸»æµç¨‹ï¼šåŒæ—¶è·‘ SEO + GEO çˆ¬å–
# ================================================================
def load_prompts_from_excel(excel_file: str):
    if not os.path.exists(excel_file):
        print(f"âŒ æ‰¾ä¸åˆ° Excel æ–‡ä»¶ï¼š{excel_file}")
        return []

    try:
        df = pd.read_excel(excel_file, engine="openpyxl")
    except Exception:
        df = pd.read_excel(excel_file)  # å…œåº•

    # ä¼˜å…ˆæ‰¾åä¸º "Prompt" çš„åˆ—ï¼Œå¦åˆ™é»˜è®¤ç”¨ç¬¬äºŒåˆ—
    if "Prompt" in df.columns:
        prompts = df["Prompt"].dropna().tolist()
    else:
        prompts = df.iloc[:, 1].dropna().tolist()

    print(f"ğŸ“‘ ä» {excel_file} åŠ è½½åˆ° {len(prompts)} æ¡ Prompt")
    return prompts


def run_crawling():
    prompts = load_prompts_from_excel(EXCEL_FILE)
    if not prompts:
        return

    seo_logs = []
    geo_logs = []

    for idx, prompt in enumerate(prompts, 1):
        print("\n" + "=" * 70)
        print(f"ğŸ¯ Query {idx}/{len(prompts)}: {prompt}")
        print("=" * 70)

        query_id = safe_query_name(prompt)

        # ------------------------------------------------------------------
        # 1) SEO æ¸ é“ï¼šEtsy æœç´¢
        # ------------------------------------------------------------------
        seo_query_dir = os.path.join(SEO_BASE_DIR, query_id)
        os.makedirs(seo_query_dir, exist_ok=True)

        seo_urls, seo_search_url = get_seo_urls_from_etsy(prompt)
        if not seo_urls:
            print(f"âš ï¸ [SEO] æœªæå–åˆ°å•†å“é“¾æ¥ï¼Œè·³è¿‡è¯¥ query çš„ SEO éƒ¨åˆ†")
        else:
            for rank, url in enumerate(seo_urls, 1):
                if rank > MAX_LISTINGS_PER_QUERY:
                    break
                log_row = fetch_with_firecrawl(
                    url=url,
                    out_dir=seo_query_dir,
                    channel="SEO",
                    query_id=query_id,
                    rank=rank
                )
                log_row["prompt"] = prompt
                log_row["search_url"] = seo_search_url
                seo_logs.append(log_row)
                # ç¤¼è²Œæ€§ç­‰å¾…ï¼Œé¿å…è¿‡å¿«è¯·æ±‚
                time.sleep(1)

        # ------------------------------------------------------------------
        # 2) GEO æ¸ é“ï¼šOpenAI WebSearch
        # ------------------------------------------------------------------
        geo_query_dir = os.path.join(GEO_BASE_DIR, query_id)
        os.makedirs(geo_query_dir, exist_ok=True)

        geo_urls, geo_raw_text = get_geo_urls_from_openai(prompt)
        if not geo_urls:
            print(f"âš ï¸ [GEO] æœªè·å¾— GPT è¿”å›çš„å•†å“é“¾æ¥ï¼Œè·³è¿‡è¯¥ query çš„ GEO éƒ¨åˆ†")
        else:
            # å¯é€‰ï¼šä¿å­˜ GPT åŸå§‹è¾“å‡ºï¼Œæ–¹ä¾¿åˆ†æ
            raw_out_path = os.path.join(geo_query_dir, "gpt_raw_output.txt")
            try:
                with open(raw_out_path, "w", encoding="utf-8") as f:
                    f.write("PROMPT:\n" + prompt + "\n\n")
                    f.write("GPT RAW OUTPUT:\n\n" + geo_raw_text)
                print(f"ğŸ“ [GEO] å·²ä¿å­˜ GPT åŸå§‹è¾“å‡ºåˆ° {raw_out_path}")
            except Exception as e:
                print(f"âš ï¸ [GEO] ä¿å­˜ GPT åŸå§‹è¾“å‡ºå¤±è´¥: {e}")

            for rank, url in enumerate(geo_urls, 1):
                if rank > MAX_LISTINGS_PER_QUERY:
                    break
                log_row = fetch_with_firecrawl(
                    url=url,
                    out_dir=geo_query_dir,
                    channel="GEO",
                    query_id=query_id,
                    rank=rank
                )
                log_row["prompt"] = prompt
                geo_logs.append(log_row)
                time.sleep(1)

    # ================================================================
    # ä¿å­˜ä¸¤ä¸ªæ¸ é“çš„æŠ“å–æ—¥å¿—
    # ================================================================
    if seo_logs:
        df_seo = pd.DataFrame(seo_logs)
        seo_log_path = os.path.join(SEO_BASE_DIR, "seo_crawl_summary.csv")
        df_seo.to_csv(seo_log_path, index=False, encoding="utf-8-sig")
        print(f"\nâœ… [SEO] æŠ“å–æ—¥å¿—å·²ä¿å­˜: {seo_log_path}")

    if geo_logs:
        df_geo = pd.DataFrame(geo_logs)
        geo_log_path = os.path.join(GEO_BASE_DIR, "geo_crawl_summary.csv")
        df_geo.to_csv(geo_log_path, index=False, encoding="utf-8-sig")
        print(f"âœ… [GEO] æŠ“å–æ—¥å¿—å·²ä¿å­˜: {geo_log_path}")

    print("\nğŸ‰ çˆ¬å–æµç¨‹å®Œæˆï¼ˆSEO + GEOï¼‰")
    print(f"   SEO æ¡æ•°: {len(seo_logs)}")
    print(f"   GEO æ¡æ•°: {len(geo_logs)}")


# ================================================================
# è„šæœ¬å…¥å£
# ================================================================
if __name__ == "__main__":
    run_crawling()
