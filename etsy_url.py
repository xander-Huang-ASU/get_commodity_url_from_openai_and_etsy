import os
import re
import time
import json
import csv
import pandas as pd
from datetime import datetime
from dotenv import load_dotenv
from firecrawl import Firecrawl

# ========== âš™ï¸ ç¯å¢ƒé…ç½® ==========
load_dotenv()

FIRECRAWL_API_KEY = ""
# if not FIRECRAWL_API_KEY:
#     raise ValueError("âŒ è¯·å…ˆåœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® FIRECRAWL_API_KEY")

# ========== âš™ï¸ å¸¸é‡é…ç½® ==========
EXCEL_FILE = "prompts.xlsx"
OUTPUT_FILE = "outputs_etsy/etsy_firecrawl_summary.csv"
BASE_DIR = "outputs_etsy"
MAX_URLS = 3
MAX_RETRIES = 3
RETRY_DELAY = 3  # ç§’
TIMEOUT = 300000    # Firecrawlè¶…æ—¶ï¼ˆæ¯«ç§’ï¼‰

os.makedirs(BASE_DIR, exist_ok=True)
firecrawl = Firecrawl(api_key=FIRECRAWL_API_KEY)


# ========== Firecrawl æŠ“å–å‡½æ•° ==========
def fetch_with_firecrawl(url, prompt_dir):
    import random
    safe_name = re.sub(r"[^a-zA-Z0-9_-]", "_", url[:80])
    md_path = os.path.join(prompt_dir, f"{safe_name}.md")
    html_path = os.path.join(prompt_dir, f"{safe_name}.html")
    json_path = os.path.join(prompt_dir, f"{safe_name}_full.json")

    base_timeout = TIMEOUT
    for attempt in range(1, MAX_RETRIES + 1):
        print(f"\nğŸ•¸ï¸ Firecrawl æŠ“å–ç¬¬ {attempt}/{MAX_RETRIES} æ¬¡: {url}")
        try:
            start = time.time()

            result = firecrawl.scrape(
                url,
                formats=[
                    "markdown",
                    "html",
                    {
                        "type": "json",
                        "schema": {"type": "object", "properties": {}}
                    }
                ],
                only_main_content=False,
                timeout=base_timeout
            )
            elapsed = round(time.time() - start, 2)

            if result.html and "Please enable JS" in result.html:
                print("âš ï¸ æ£€æµ‹åˆ°åçˆ¬é¡µé¢ï¼ˆéœ€è¦å¯ç”¨ JSï¼‰ï¼Œå‡†å¤‡å»¶è¿Ÿåé‡è¯•...")
                delay = random.uniform(10, 20)
                print(f"â³ ç­‰å¾… {round(delay, 1)} ç§’å†è¯•...")
                time.sleep(delay)
                continue

            with open(md_path, "w", encoding="utf-8") as f:
                f.write(result.markdown or "")
            with open(html_path, "w", encoding="utf-8") as f:
                f.write(result.html or "")
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(result.json or {}, f, ensure_ascii=False, indent=2)

            print(f"âœ… æŠ“å–æˆåŠŸ [{elapsed}s] å·²ä¿å­˜ Markdown / HTML / JSON")
            return {
                "url": url,
                "status": "success",
                "markdown_file": md_path,
                "html_file": html_path,
                "json_file": json_path,
                "attempt": attempt,
                "elapsed_s": elapsed,
                "time_s": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }

        except Exception as e:
            print(f"âŒ Firecrawl æŠ“å–å¤±è´¥: {e}")
            delay = RETRY_DELAY * attempt + random.uniform(2, 5)
            base_timeout = int(base_timeout * 1.5)
            print(f"âš™ï¸ ç¬¬ {attempt} æ¬¡å¤±è´¥ï¼Œ{round(delay, 1)} ç§’åé‡è¯•ï¼Œæ–°çš„ timeout={base_timeout/1000:.1f}s")
            time.sleep(delay)

    print("âŒ Firecrawl å…¨éƒ¨é‡è¯•å¤±è´¥")
    return {"url": url, "status": "fail"}


# ========== ä¸»æµç¨‹ï¼ˆåªæŠ“å•†å“é¡µï¼‰ ==========
def main():
    if not os.path.exists(EXCEL_FILE):
        print("âŒ æœªæ‰¾åˆ° Excel æ–‡ä»¶ã€‚")
        return

    df = pd.read_excel(EXCEL_FILE, engine="openpyxl")
    prompts = df["Prompt"].dropna().tolist() if "Prompt" in df.columns else df.iloc[:, 1].dropna().tolist()
    all_logs = []

    for idx, prompt in enumerate(prompts, 1):
        print(f"\n========== ğŸ” Prompt {idx}/{len(prompts)} ==========")
        prompt_name = re.sub(r"[^a-zA-Z0-9_()ï¼ˆï¼‰\u4e00-\u9fa5-]", "_", prompt[:50])
        prompt_dir = os.path.join(BASE_DIR, prompt_name)
        os.makedirs(prompt_dir, exist_ok=True)

        # âœ… Etsy æœç´¢ URL
        search_url = f"https://www.etsy.com/search?q={prompt.replace(' ', '+')}"
        print(f"\nğŸ” Etsy æœç´¢ï¼š{search_url}")

        # ç”¨ Firecrawl æŠ“å– HTML ä½†ä¸ä¿å­˜æ–‡ä»¶ï¼Œåªç”¨äºæå–é“¾æ¥
        try:
            result = firecrawl.scrape(
                search_url,
                formats=["html"],
                only_main_content=False,
                timeout=TIMEOUT
            )
            html = result.html or ""
            urls = re.findall(r'https://www\.etsy\.com/listing/[0-9]+/[^\s"\'<>]+', html)
            urls = list(dict.fromkeys(urls))[:MAX_URLS]
            print(f"ğŸ”— æå–åˆ° {len(urls)} ä¸ªå•†å“é“¾æ¥")

        except Exception as e:
            print(f"âŒ Etsy æœç´¢æŠ“å–å¤±è´¥: {e}")
            continue

        if not urls:
            print("âš ï¸ æœªæå–åˆ°å•†å“é“¾æ¥ï¼Œè·³è¿‡ã€‚")
            continue

        # âœ… åªæŠ“å–å•†å“è¯¦æƒ…é¡µ
        for url in urls:
            result = fetch_with_firecrawl(url, prompt_dir)
            result.update({
                "prompt": prompt,
                "prompt_dir": prompt_dir,
                "search_url": search_url
            })
            all_logs.append(result)
            time.sleep(1)

    # è¾“å‡ºç»“æœ CSV
    df_out = pd.DataFrame(all_logs)
    df_out.to_csv(OUTPUT_FILE, index=False, encoding="utf-8-sig", quoting=csv.QUOTE_ALL)
    print(f"\nâœ… å·²ä¿å­˜ç»“æœï¼š{OUTPUT_FILE}")
    print(f"ğŸ“ å„ prompt æ–‡ä»¶ä¿å­˜åœ¨ï¼š{BASE_DIR}")


if __name__ == "__main__":
    main()
